{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methylation Biomarkers for Predicting Cancer**\n",
    "\n",
    "## **Dimensionality Reduction: Principal Component Anlaysis**\n",
    "\n",
    "**Author:** Meg Hutch\n",
    "\n",
    "**Date:** January 26, 2020\n",
    "\n",
    "**Objective:** Use neural networks to classify colon, esophagus, liver, and stomach cancer patients and healthy subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Training, Testing, and Principal component data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "mcTrain = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/mcTrain.csv')\n",
    "# Testing set\n",
    "mcTest = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/mcTest.csv')\n",
    "# All Principal Components\n",
    "principal_Df_ALL = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/principalDF_ALL.csv')\n",
    "# Principal Components that make up 90% of the variance of the training set\n",
    "genesTrain_transformed_90 = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/genesTrain_transformed_90.csv')\n",
    "# Principal Components projected onto the test set\n",
    "genesTest_transformed_90 = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/genesTest_transformed_90.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Process Data**\n",
    "\n",
    "* Standarized all data: UPDATE: Not going to worry about this yet!\n",
    "* Make sure that data is formatted correctly\n",
    "* Structure the neural network architecture for multi-classifciation - (check loss function?)\n",
    "* Determine how to do LOOCFV \n",
    "* The idea is that I will try and get high AUCs using the LOOCFV and then once I optimize that, I'll test on the testing set (is it cheating at all to test and then go back to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove genetic data from the mcTrain dataset\n",
    "mcTrain = mcTrain[['seq_num','diagnosis', 'dilute_library_concentration', 'age', 'gender']]\n",
    "\n",
    "# do the same for the testing set\n",
    "mcTest = mcTest[['seq_num','diagnosis', 'dilute_library_concentration', 'age', 'gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the first column name of the PC dataframes\n",
    "genesTrain_transformed_90.rename(columns={'Unnamed: 0':'seq_num'}, inplace=True)\n",
    "genesTest_transformed_90.rename(columns={'Unnamed: 0':'seq_num'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge PCs with clinical/phenotypic data\n",
    "mcTrain = pd.merge(mcTrain, genesTrain_transformed_90, how=\"left\", on=\"seq_num\") \n",
    "mcTest = pd.merge(mcTest, genesTest_transformed_90, how=\"left\", on=\"seq_num\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Labels (Diagnosis) from the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain.drop(columns=[\"diagnosis\"])\n",
    "mcTest_x = mcTest.drop(columns=[\"diagnosis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Labeled Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_y = mcTrain[['seq_num', 'diagnosis']]\n",
    "mcTest_y = mcTest[['seq_num', 'diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert seq_num id to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.set_index('seq_num')\n",
    "mcTrain_y = mcTrain_y.set_index('seq_num')\n",
    "\n",
    "mcTest_x = mcTest_x.set_index('seq_num')\n",
    "mcTest_y = mcTest_y.set_index('seq_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Network Cancer Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch packages\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Process Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace Categorical Outputs to Numeric Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the unique target variables\n",
    "mcTrain_y.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace each outcome target with numerical value\n",
    "mcTrain_y = mcTrain_y.replace('HEA', 1)\n",
    "mcTrain_y = mcTrain_y.replace('CRC', 2)\n",
    "mcTrain_y = mcTrain_y.replace('ESCA', 3)\n",
    "mcTrain_y = mcTrain_y.replace('HCC', 4)\n",
    "mcTrain_y = mcTrain_y.replace('STAD', 5)\n",
    "\n",
    "mcTest_y = mcTest_y.replace('HEA', 1)\n",
    "mcTest_y = mcTest_y.replace('CRC', 2)\n",
    "mcTest_y = mcTest_y.replace('ESCA', 3)\n",
    "mcTest_y = mcTest_y.replace('HCC', 4)\n",
    "mcTest_y = mcTest_y.replace('STAD', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into arrays\n",
    "xb = np.array(mcTrain_x, dtype = \"float32\")\n",
    "yb = np.array(mcTrain_y, dtype = \"float32\")\n",
    "\n",
    "# Convert arrays into tensors\n",
    "xb = torch.from_numpy(xb)\n",
    "yb = torch.from_numpy(yb)\n",
    "\n",
    "# Combine the arrays\n",
    "trainloader = TensorDataset(xb, yb)\n",
    "\n",
    "# Define the batchsize\n",
    "batch_size = 32\n",
    "\n",
    "# Training Loader\n",
    "trainloader = DataLoader(trainloader, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into arrays\n",
    "xb = np.array(mcTest_x, dtype = \"float32\")\n",
    "yb = np.array(mcTest_y, dtype = \"float32\")\n",
    "\n",
    "# Convert arrays into tensors\n",
    "xb = torch.from_numpy(xb)\n",
    "yb = torch.from_numpy(yb)\n",
    "\n",
    "# Combine the arrays\n",
    "testloader = TensorDataset(xb, yb) \n",
    "\n",
    "# Define the batchsize\n",
    "batch_size= 32\n",
    "\n",
    "# Training Loader\n",
    "testloader = DataLoader(testloader, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Neural Network Model**\\\n",
    "\n",
    "# **TO DOs:**\n",
    "\n",
    "* LOOCFV!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with hidden layers - 50 inputs\n",
    "model = nn.Sequential(nn.Linear(50, 30),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(30, 5))\n",
    "                      \n",
    "                      \n",
    "# Set optimizer and learning rate\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #don't use with softmax or sigmoid- PyTorch manual indicates \"This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\"\n",
    "\n",
    "# Set epochs\n",
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for xb, yb in trainloader:\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training pass\n",
    "        output = model.forward(xb)\n",
    "        loss = criterion(output, yb) # Loss calculated from the output compared to the labels  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
    "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
