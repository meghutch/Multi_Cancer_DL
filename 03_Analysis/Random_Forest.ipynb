{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methylation Biomarkers for Predicting Cancer**\n",
    "\n",
    "## **Random Forest for Feature Selection**\n",
    "\n",
    "**Author:** Meg Hutch\n",
    "\n",
    "**Date:** February 14, 2020\n",
    "\n",
    "**Objective:** Use random forest to select genes for features in our deep learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_recall_fscore_support, f1_score, log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory for git hub\n",
    "import os\n",
    "os.chdir('/home/mrh1996/')\n",
    "#os.chdir('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/')\n",
    "os. getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain = pd.read_csv('Multi_Cancer_DL/02_Processed_Data/mcTrain_70_30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Un-neccessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain = mcTrain.drop(columns=[\"dilute_library_concentration\", \"age\", \"gender\", \"frag_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data into X inputs and Y outputs (diagnosis classification)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain.drop(columns=[\"diagnosis\"])\n",
    "mcTrain_y = mcTrain[['seq_num','diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code the Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace each outcome target with numerical value\n",
    "mcTrain_y = mcTrain_y.replace('HEA', 0)\n",
    "mcTrain_y = mcTrain_y.replace('CRC', 1)\n",
    "mcTrain_y = mcTrain_y.replace('ESCA', 2)\n",
    "mcTrain_y = mcTrain_y.replace('HCC', 3)\n",
    "mcTrain_y = mcTrain_y.replace('STAD', 4)\n",
    "mcTrain_y = mcTrain_y.replace('GBM', 5)\n",
    "mcTrain_y = mcTrain_y.replace('BRCA', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert seq_num id to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.set_index('seq_num')\n",
    "mcTrain_y = mcTrain_y.set_index('seq_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training Data into a training/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(21420)\n",
    "X_train, X_test, y_train, y_test = train_test_split(mcTrain_x, mcTrain_y, test_size=0.25, random_state=25, shuffle = True, stratify = mcTrain_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Disease Distributions After Training/Testing Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perc = y_train.groupby(['diagnosis']).size()/len(y_train)*100\n",
    "y_test_perc = y_test.groupby(['diagnosis']).size()/len(y_test)*100\n",
    "\n",
    "#print(y_train_perc)\n",
    "#print(y_test_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encode y classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "y_train_multi = preprocessing.label_binarize(y_train, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "y_test_multi = preprocessing.label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save copy of X_train - this will be used for feature selection down the line\n",
    "X_train_orig = X_train\n",
    "\n",
    "# Convert all to arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values \n",
    "y_test = y_test.values\n",
    "\n",
    "# Convert y_train to 1D\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# test\n",
    "#print(y_test.reshape(1,-1)) \n",
    "\n",
    "# Feb 22 test\n",
    "#y_train = y_train.reshape(1,-1)\n",
    "#y_test = y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape\n",
    "#y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "The hyperparameter tuning function was adapted from Garrett's modeling lecture:\n",
    "\n",
    "https://github.com/geickelb/HSIP442_guest_lecture/blob/master/notebooks/modeling.ipynb\n",
    "\n",
    "scoring parameter for multi-classification: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter (will try f1_samples and precision_samples and/or just accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Random Forest Hypertuning Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning_fxn(X, y, nfolds, model , param_grid, scoring = 'accuracy', verbose=False): \n",
    "    \"\"\"function that uses GridSearchCV to test a specified param_grid of hyperparameters and choose the optimal one based on nfolds cross-validation results. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    X -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import KFold, GridSearchCV\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator= model,\n",
    "                                     param_grid=param_grid,\n",
    "                                     cv=KFold(nfolds),\n",
    "                                     scoring=scoring,\n",
    "                                     return_train_score=True,\n",
    "                                     n_jobs = -1)\n",
    "\n",
    "    #OneVsRestClassifier(grid_search.fit(X, y))   \n",
    "    grid_search.fit(X, y)\n",
    "    print(\" scorer function: {}\".format(scoring))\n",
    "    print(\" ##### CV performance: mean & sd scores #####\")\n",
    "\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    print('best cv score: {:0.3f}'.format(grid_search.best_score_))\n",
    "    print('best cv params: ', grid_search.best_params_)\n",
    "\n",
    "    worst_index=np.argmin(grid_search.cv_results_['mean_test_score'])\n",
    "    print('worst cv score: {:0.3f}'.format(grid_search.cv_results_['mean_test_score'][worst_index]))\n",
    "    print('worst cv params: ', grid_search.cv_results_['params'][worst_index])\n",
    "    ##\n",
    "    if verbose==True:\n",
    "        for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "\n",
    "    return(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning RF hyperparameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [10] #  100, 300, 500, 1000\n",
    "# Number of features to consider at every split\n",
    "max_features = [3, 10, 'auto'] # 'auto' which is equivalent to sqrt(n_features)\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 5, 10, 15]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper=hypertuning_fxn(X_train, y_train, nfolds=10, model=model , param_grid=param_grid, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the Best Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyper.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x, y, cv=True):\n",
    "    \"\"\"prints common binary classification evaluation metrics and an ROC curve. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    x -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import log_loss, average_precision_score, precision_recall_curve\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    if cv==True:\n",
    "        cv_results= cross_val_score(model, x, y, scoring='roc_auc', cv=10)\n",
    "        print(\"across 10 fold cv on trainingset, the model had \\n\", \n",
    "             \"mean auroc: {:0.3f}\".format(np.mean(cv_results)), \"\\n\",\n",
    "             \"std auroc: {:0.3f}\".format(np.std(cv_results))\n",
    "             )\n",
    "\n",
    "        base_cv_score=np.mean(cross_val_score(model, x, y, scoring='roc_auc', cv=10)) \n",
    "\n",
    "    \n",
    "    print(\"###metrics on provided dataset:###\")\n",
    "    ##basic model performance\n",
    "    y_hat = model.predict(x) # predicted classes using default 0.5 threshold\n",
    "    y_proba = model.predict_proba(x)[:, 1] #predicted probabilities\n",
    "    errors = abs(y_hat - y)\n",
    "    mape = 100 * np.mean(errors / y) # mean absolute percentage error\n",
    "    accuracy = 100 - mape \n",
    "    auc=roc_auc_score(y, y_proba, multi_class = 'ovr', average = 'macro')\n",
    "    loss= log_loss(y, y_hat)\n",
    "\n",
    "    print ('the AUC is: {:0.3f}'.format(auc))\n",
    "    print ('the logloss is: {:0.3f}'.format(loss))\n",
    "    print(\"confusion matrix:\\n \", confusion_matrix(y, y_hat))\n",
    "    print(\"classification report:\\n \", classification_report(y,y_hat, digits=3))\n",
    "\n",
    "    ez_roc(model, x, y, pos_label=1) #plotting roc curve\n",
    "    plt.show()\n",
    "    ez_prc(model, x, y, pos_label=1) #plotting roc curve\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_hyper.best_estimator_,X_test,y_test.ravel(), cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape\n",
    "#y_test.ravel().shape\n",
    "#X_train.shape\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier   \n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(rf_hyper.best_estimator_.fit(X_train, y_train))\n",
    "#classifier = OneVsRestClassifier(rf.fit(X_train, y_train))\n",
    "#yprob = clf.predict(X_test)\n",
    "#clf.fit.predict_proba(X_test)\n",
    "\n",
    "#clf = OneVsRestClassifier(rf()).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n",
    "                                  average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = multi_target_forest.fit(X_train, y_train)\n",
    "#rf = rf_hyper.best_estimator_fit(X_train, y_train)\n",
    "predictions = rf_hyper.best_estimator_.predict(X_test)\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(predictions - y_test)\n",
    "#Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)\n",
    "#roc_auc_score(y_test, predictions, multi_class = 'ovr') # multi_class must be in ('ovo', 'ovr')\n",
    "clf = OneVsRestClassifier(rf_hyper.best_estimator_.fit(X_train, y_train))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf_preds = clf.predict(X_test)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rf_hyper.classes_)\n",
    "rf_probs = rf_hyper.best_estimator_.predict_proba(X_test)[:,]\n",
    "rf_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Computes the AUC of each class against the rest [3] [4]. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when average == 'macro', because class imbalance affects the composition of each of the ‘rest’ groupings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, rf_probs, multi_class = 'ovr', average = 'macro') # multi_class must be in ('ovo', 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, rf_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "plt.title('ROC curve')\n",
    "ax1= plt.plot(fpr, tpr, 'b', label = '%s AUC = %0.3f' % (model_name, roc_auc), linewidth=2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_hyper.best_estimator_,X_test,y_test, cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities for each class - this shows how predictions were made - not sure we need to do much more here\n",
    "rf_probs = rf_hyper.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "# convert to an array\n",
    "rf_probs = np.asarray(rf_probs)\n",
    "# this helps reduce the list\n",
    "#rf_probs = np.amax(rf_probs, axis=0)\n",
    "# convert to a dataframe\n",
    "#rf_probs = pd.DataFrame(rf_probs)\n",
    "#print(rf_probs)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "#errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "#print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "#f1_score(y_test, predictions, average = 'samples') \n",
    "\n",
    "#accuracy_score(y_test, predictions)\n",
    "#roc_auc_score(y_test, predictions) # multi_class must be in ('ovo', 'ovr')\n",
    "clf = OneVsRestClassifier(rf.fit(X_train, y_train))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf_preds = clf.predict_proba(X_test)[: 1]\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_preds2 = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_preds2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_probsx = np.array(rf_probs)\n",
    "#rf_probsx = rf_probsx.ravel()\n",
    "rf_probsx.shape\n",
    "#predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to 1D\n",
    "#y_test = y_test.ravel()\n",
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_preds = clf_preds.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.ravel()\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Important Features**\n",
    "\n",
    "Feature Importance for Multi-class classification:\n",
    "https://stackoverflow.com/questions/54562464/can-i-show-feature-importance-for-multioutputclassifier\n",
    "\n",
    "MultiOutputClassifier objects have an attribute called estimators_. If you run multi_forest.estimators_, you will get a list containing an object for each of your RandomForest classifiers.\n",
    "\n",
    "For each of these RandomForest classifier objects, you can access its feature importances through the feature_importances_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "feat_impts = [] \n",
    "# bind all rf estimators for each classifier (each multi-class output - in our case 7)\n",
    "for clf in rf.estimators_:\n",
    "    feat_impts.append(clf.feature_importances_)\n",
    "\n",
    "# calculate the mean of features across genes\n",
    "feat = np.mean(feat_impts, axis=0)\n",
    "# create a list of features (gene names)\n",
    "features = list(X_train_orig.columns.values) \n",
    "# add gene names to the means\n",
    "feat_importances = pd.Series(feat, index=X_train_orig.columns)  \n",
    "\n",
    "# plot feature importance for nlargest means \n",
    "feat_importances.nlargest(25).plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p31049",
   "language": "python",
   "name": "p31049"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
