{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methylation Biomarkers for Predicting Cancer**\n",
    "\n",
    "## **Random Forest for Feature Selection**\n",
    "\n",
    "**Author:** Meg Hutch\n",
    "\n",
    "**Date:** February 14, 2020\n",
    "\n",
    "**Objective:** Use random forest to select genes for features in our deep learning classifier.\n",
    "\n",
    "**Update**: Could try and choose a classifier maybe based off highest AUC and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_score, recall_score, precision_recall_fscore_support, f1_score, log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Box Sync\\\\Projects'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set working directory for git hub\n",
    "import os\n",
    "#os.chdir('/home/mrh1996/')\n",
    "os.chdir('C:\\\\Users\\\\User\\\\Box Sync/Projects/')\n",
    "os. getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = pd.read_csv('Multi_Cancer_DL/02_Processed_Data/Final_Datasets/mcTrain_x_Full_70_30.csv')\n",
    "mcTrain_y = pd.read_csv('Multi_Cancer_DL/02_Processed_Data/Final_Datasets/mcTrain_y_Full_70_30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Un-neccessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.drop(columns=[\"dilute_library_concentration\", \"age\", \"gender\", \"frag_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert seq_num id to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.set_index('id')\n",
    "mcTrain_y = mcTrain_y.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training Data into a training/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(21420)\n",
    "X_train, X_test, y_train, y_test = train_test_split(mcTrain_x, mcTrain_y, test_size=0.25, random_state=25, shuffle = True, stratify = mcTrain_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Disease Distributions After Training/Testing Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perc = y_train.groupby(['diagnosis']).size()/len(y_train)*100\n",
    "y_test_perc = y_test.groupby(['diagnosis']).size()/len(y_test)*100\n",
    "\n",
    "#print(y_train_perc)\n",
    "#print(y_test_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encode y classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "y_train_multi = preprocessing.label_binarize(y_train, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "y_test_multi = preprocessing.label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save copy of X_train - this will be used for feature selection down the line\n",
    "X_train_orig = X_train\n",
    "\n",
    "# Convert all to arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values \n",
    "y_test = y_test.values\n",
    "\n",
    "# Convert y_train to 1D\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# test\n",
    "#print(y_test.reshape(1,-1)) \n",
    "\n",
    "# Feb 22 test\n",
    "#y_train = y_train.reshape(1,-1)\n",
    "#y_test = y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "The hyperparameter tuning function was adapted from Garrett's modeling lecture:\n",
    "\n",
    "https://github.com/geickelb/HSIP442_guest_lecture/blob/master/notebooks/modeling.ipynb\n",
    "\n",
    "scoring parameter for multi-classification: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter (will try f1_samples and precision_samples and/or just accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define scoring function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring = ['AUC', 'Accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Random Forest Hypertuning Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning_fxn(X, y, nfolds, model , param_grid, scoring = 'roc_auc_ovo', verbose=True, cv = True, \n",
    "                    return_train_score = True): \n",
    "    \"\"\"function that uses GridSearchCV to test a specified param_grid of hyperparameters and choose the optimal one based on nfolds cross-validation results. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    X -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(12345)\n",
    "    \n",
    "    # The scorers can be either be one of the predefined metric strings or a scorer\n",
    "    # callable, like the one returned by make_scorer\n",
    "    #scoring = {'AUC': 'roc_auc_ovr', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator= model,\n",
    "                                     param_grid=param_grid,\n",
    "                                     cv=StratifiedKFold(nfolds), # stratified k-folds will preserve class balances - this function is what got the rest of the code to work with roc validation\n",
    "                                     scoring=scoring,\n",
    "                                     return_train_score=True,\n",
    "                                     n_jobs = -1)\n",
    "    \n",
    "    #scoring = {'accuracy': 'accuracy', 'auc': 'roc_auc_ovr'}\n",
    "    #scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "    \n",
    "    #OneVsRestClassifier(grid_search.fit(X, y))   \n",
    "    grid_search.fit(X, y)\n",
    "    print(\" scorer function: {}\".format(scoring))\n",
    "    print(\" ##### CV performance: mean & sd scores #####\")\n",
    "\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    \n",
    "    print('best cv score: {:0.3f}'.format(grid_search.best_score_))\n",
    "    print('best cv params: ', grid_search.best_params_)\n",
    "\n",
    "    worst_index=np.argmin(grid_search.cv_results_['mean_test_score'])\n",
    "    print('worst cv score: {:0.3f}'.format(grid_search.cv_results_['mean_test_score'][worst_index]))\n",
    "    print('worst cv params: ', grid_search.cv_results_['params'][worst_index])\n",
    "    ##\n",
    "    if verbose==True:\n",
    "        for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    \n",
    "    return(grid_search)\n",
    "    #print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'roc_auc_ovo' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[1;34m(scoring)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSCORERS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'roc_auc_ovo'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6ca15ff8f8f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mrf_hyper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypertuning_fxn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'roc_auc_ovo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-0b693b3d3345>\u001b[0m in \u001b[0;36mhypertuning_fxn\u001b[1;34m(X, y, nfolds, model, param_grid, scoring, verbose, cv, return_train_score)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#OneVsRestClassifier(grid_search.fit(X, y))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" scorer function: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" ##### CV performance: mean & sd scores #####\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[1;32m--> 654\u001b[1;33m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    341\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[0;32m    342\u001b[0m                                                           six.string_types):\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mscorers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    271\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[1;34m(scoring)\u001b[0m\n\u001b[0;32m    231\u001b[0m             raise ValueError('%r is not a valid scoring value. '\n\u001b[0;32m    232\u001b[0m                              \u001b[1;34m'Use sorted(sklearn.metrics.SCORERS.keys()) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                              'to get valid options.' % (scoring))\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'roc_auc_ovo' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options."
     ]
    }
   ],
   "source": [
    "### tuning RF hyperparameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100, 300, 500, 1000] #  100, 300, 500, 1000\n",
    "# Number of features to consider at every split\n",
    "max_features = [3, 10, 'auto'] # 'auto' which is equivalent to sqrt(n_features)\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 5, 10, 15]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper=hypertuning_fxn(X_train, y_train, nfolds=10, model=model , param_grid=param_grid, scoring= 'roc_auc_ovo', cv = True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the Best Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_hyper.best_estimator_)\n",
    "rf = rf_hyper.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rf_hyper.classes_) # number of classes - this is an attribute of the rf classifier\n",
    "\n",
    "# predict the probabilities of the classifier when applied to the test set - Note: [:,] shapes data into the right format for multiclass\n",
    "rf_probs = rf_hyper.best_estimator_.predict_proba(X_test)[:,]\n",
    "rf_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_hyper.best_estimator_.predict(X_test)[:,]\n",
    "#rf_preds = rf_preds.reshape(-1,1).shape\n",
    "rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** ovr computes the AUC of each class against the rest [3] [4]. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when average == 'macro', because class imbalance affects the composition of each of the ‘rest’ groupings\"\n",
    "\n",
    "Weighted: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc_score(y_test, rf_probs, multi_class = 'ovr', average = 'weighted') # multi_class must be in ('ovo', 'ovr')\n",
    "\n",
    "roc_auc_score(y_test_multi, rf_probs, multi_class = 'ovo') # multi_class must be in ('ovo', 'ovr')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_hyper.best_estimator_.predict(X_test)[:,]\n",
    "confusion_matrix(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['HEA', 'CRC', 'ESCA', 'HCC', 'STAD', 'GBM', 'BRCA']\n",
    "print(classification_report(y_test, rf_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Important Features**\n",
    "\n",
    "Feature Importance for Multi-class classification:\n",
    "https://stackoverflow.com/questions/54562464/can-i-show-feature-importance-for-multioutputclassifier\n",
    "\n",
    "MultiOutputClassifier objects have an attribute called estimators_. If you run multi_forest.estimators_, you will get a list containing an object for each of your RandomForest classifiers.\n",
    "\n",
    "For each of these RandomForest classifier objects, you can access its feature importances through the feature_importances_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "feat_impts = [] \n",
    "# bind all rf estimators for each classifier (each multi-class output - in our case 7)\n",
    "for clf in rf.estimators_:\n",
    "    feat_impts.append(clf.feature_importances_)\n",
    "\n",
    "# calculate the mean of features across genes\n",
    "feat = np.mean(feat_impts, axis=0)\n",
    "# create a list of features (gene names)\n",
    "features = list(X_train_orig.columns.values) \n",
    "# add gene names to the means\n",
    "feat_importances = pd.Series(feat, index=X_train_orig.columns)  \n",
    "\n",
    "# plot feature importance for nlargest means \n",
    "feat_importances.nlargest(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save List of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_100 = feat_importances.nlargest(100)\n",
    "feat_100\n",
    "feat_100.to_csv('Multi_Cancer_DL/02_Processed_Data/Final_Datasets/rf_100feats_FULL_70_30.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
