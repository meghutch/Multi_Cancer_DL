{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methylation Biomarkers for Predicting Cancer**\n",
    "\n",
    "## **Random Forest for Feature Selection**\n",
    "\n",
    "**Author:** Meg Hutch\n",
    "\n",
    "**Date:** February 14, 2020\n",
    "\n",
    "**Objective:** Use random forest to select genes for features in our deep learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_recall_fscore_support, f1_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain = pd.read_csv('C:\\\\Users\\\\User\\\\Box Sync/Projects/Multi_Cancer_DL/02_Processed_Data/mcTrain_70_30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Un-neccessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain = mcTrain.drop(columns=[\"dilute_library_concentration\", \"age\", \"gender\", \"frag_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data into X inputs and Y outputs (diagnosis classification)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain.drop(columns=[\"diagnosis\"])\n",
    "mcTrain_y = mcTrain[['seq_num','diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code the Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace each outcome target with numerical value\n",
    "mcTrain_y = mcTrain_y.replace('HEA', 0)\n",
    "mcTrain_y = mcTrain_y.replace('CRC', 1)\n",
    "mcTrain_y = mcTrain_y.replace('ESCA', 2)\n",
    "mcTrain_y = mcTrain_y.replace('HCC', 3)\n",
    "mcTrain_y = mcTrain_y.replace('STAD', 4)\n",
    "mcTrain_y = mcTrain_y.replace('GBM', 5)\n",
    "mcTrain_y = mcTrain_y.replace('BRCA', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert seq_num id to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.set_index('seq_num')\n",
    "mcTrain_y = mcTrain_y.set_index('seq_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training Data into a training/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(21420)\n",
    "X_train, X_test, y_train, y_test = train_test_split(mcTrain_x, mcTrain_y, test_size=0.25, random_state=25, shuffle = True, stratify = mcTrain_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Disease Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "0    25.414365\n",
      "1    18.784530\n",
      "2     9.944751\n",
      "3    20.441989\n",
      "4    10.497238\n",
      "5     7.734807\n",
      "6     7.182320\n",
      "dtype: float64\n",
      "diagnosis\n",
      "0    24.590164\n",
      "1    18.032787\n",
      "2     9.836066\n",
      "3    19.672131\n",
      "4    11.475410\n",
      "5     8.196721\n",
      "6     8.196721\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_train_perc = y_train.groupby(['diagnosis']).size()/len(y_train)*100\n",
    "y_test_perc = y_test.groupby(['diagnosis']).size()/len(y_test)*100\n",
    "\n",
    "print(y_train_perc)\n",
    "print(y_test_perc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "The below code was adapted from Garrett's modeling lecture:\n",
    "https://github.com/geickelb/HSIP442_guest_lecture/blob/master/notebooks/modeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Random Forest Hypertuning Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning_fxn(X, y, nfolds, model , param_grid, scoring='auc_roc', verbose=False): \n",
    "    \"\"\"function that uses GridSearchCV to test a specified param_grid of hyperparameters and choose the optimal one based on nfolds cross-validation results. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    X -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import KFold, GridSearchCV\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator= model,\n",
    "                                     param_grid=param_grid,\n",
    "                                     cv=KFold(nfolds),\n",
    "                                     scoring=scoring,\n",
    "                                     return_train_score=True,\n",
    "                                     n_jobs = -1)\n",
    "\n",
    "        \n",
    "    grid_search.fit(X, y)    \n",
    "    print(\" scorer function: {}\".format(scoring))\n",
    "    print(\" ##### CV performance: mean & sd scores #####\")\n",
    "\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    print('best cv score: {:0.3f}'.format(grid_search.best_score_))\n",
    "    print('best cv params: ', grid_search.best_params_)\n",
    "\n",
    "    worst_index=np.argmin(grid_search.cv_results_['mean_test_score'])\n",
    "    print('worst cv score: {:0.3f}'.format(grid_search.cv_results_['mean_test_score'][worst_index]))\n",
    "    print('worst cv params: ', grid_search.cv_results_['params'][worst_index])\n",
    "    ##\n",
    "    if verbose==True:\n",
    "        for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "   \n",
    "    return(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encode y classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "y_train = preprocessing.label_binarize(y_train, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "y_test = preprocessing.label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scorer function: roc_auc\n",
      " ##### CV performance: mean & sd scores #####\n",
      "best cv score: 0.666\n",
      "best cv params:  {'max_depth': 25, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "worst cv score: 0.587\n",
      "worst cv params:  {'max_depth': 5, 'max_features': 3, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "### tuning RF hyperparameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100] # 300, 500, 100\n",
    "# Number of features to consider at every split\n",
    "max_features = [3,10,'auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5,25]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 5]\n",
    "\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper=hypertuning_fxn(X_train, y_train, nfolds=5, model=model , param_grid=param_grid, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the Best Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=12345, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = rf_hyper.best_estimator_\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs for roc\n",
    "def ez_roc(model, x, y, pos_label=1):\n",
    "    \"\"\"prints a basic Recievor Operator Curve (ROC). \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    x -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    pos_label --binary label considered positive in y  (dtype='int', default=1)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    model_name=type(model).__name__ # defining model name as the __name__ characteristic held by sklearn models\n",
    "\n",
    "    y_proba = model.predict_proba(x)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y, y_proba, pos_label=pos_label)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('ROC curve')\n",
    "    ax1= plt.plot(fpr, tpr, 'b', label = '%s AUC = %0.3f' % (model_name, roc_auc), linewidth=2)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs for precision-recall curve\n",
    "def ez_prc(model, x, y, pos_label=1):\n",
    "    \"\"\"prints a basic Precision-Recall curve. \n",
    "    recall: the porportion of positives in the dataset that were correctly classified. (true_pos/ (true_pos + false_neg))\n",
    "    precision: the porportion of predicted y=1 values that are correct (true_pos/ (true_pos + false_pos))\n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    x -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    pos_label --binary label considered positive in y  (dtype='int', default=1)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "    model_name=type(model).__name__ # defining model name as the __name__ characteristic held by sklearn models\n",
    "\n",
    "    y_proba = model.predict_proba(x)[:,1]\n",
    "\n",
    "    precision, recall, thresholds =precision_recall_curve(y, y_proba, pos_label=1, sample_weight=None)\n",
    "    avg_p=average_precision_score(y, y_proba, pos_label=1, sample_weight=None)\n",
    "    \n",
    "    plt.title('Precision-Recall curve')\n",
    "    ax1= plt.plot(precision, recall, 'b', label = '%s AP = %0.3f' % (model_name, avg_p), linewidth=2)\n",
    "    plt.legend(loc = 'lower left')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,x,y, cv=True):\n",
    "    \"\"\"prints common binary classification evaluation metrics and an ROC curve. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    x -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import log_loss, average_precision_score, precision_recall_curve\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    if cv==True:\n",
    "        cv_results= cross_val_score(model, x, y, scoring='roc_auc', cv=10)\n",
    "        print(\"across 10 fold cv on trainingset, the model had \\n\", \n",
    "             \"mean auroc: {:0.3f}\".format(np.mean(cv_results)), \"\\n\",\n",
    "             \"std auroc: {:0.3f}\".format(np.std(cv_results))\n",
    "             )\n",
    "\n",
    "        base_cv_score=np.mean(cross_val_score(model, x, y, scoring='roc_auc', cv=10)) \n",
    "\n",
    "    print(\"###metrics on provided dataset:###\")\n",
    "    ##basic model performance\n",
    "    y_hat = model.predict(x) # predicted classes using default 0.5 threshold\n",
    "    y_proba = model.predict_proba(x)[:,1] #predicted probabilities\n",
    "    errors = abs(y_hat - y)\n",
    "    mape = 100 * np.mean(errors / y) # mean absolute percentage error\n",
    "    accuracy = 100 - mape \n",
    "    auc=roc_auc_score(y, y_proba)\n",
    "    loss= log_loss(y, y_hat)\n",
    "\n",
    "    print ('the AUC is: {:0.3f}'.format(auc))\n",
    "    print ('the logloss is: {:0.3f}'.format(loss))\n",
    "    print(\"confusion matrix:\\n \", confusion_matrix(y, y_hat))\n",
    "    print(\"classification report:\\n \", classification_report(y,y_hat, digits=3))\n",
    "\n",
    "    ez_roc(model, x, y, pos_label=1) #plotting roc curve\n",
    "    plt.show()\n",
    "    ez_prc(model, x, y, pos_label=1) #plotting roc curve\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###metrics on provided dataset:###\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7ea81341d04e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-f2d3e9ae6797>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(model, x, y, cv)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m##basic model performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# predicted classes using default 0.5 threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0my_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#predicted probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mmape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean absolute percentage error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_train,y_train, cv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RandomForests:\n",
      "###metrics on provided dataset:###\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6c05b462279f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n RandomForests:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-f2d3e9ae6797>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(model, x, y, cv)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m##basic model performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# predicted classes using default 0.5 threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0my_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#predicted probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mmape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean absolute percentage error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "print('\\n RandomForests:')\n",
    "evaluate_model(rf, X_test,y_test,cv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Important Features**\n",
    "\n",
    "**Just run based on what the best estimator ends up being?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 02.17.2020: Is selectfrommodel code neccessary? \n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "# First feature selection test \n",
    "#sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "#sel = SelectFromModel(rf_hyper.best_estimator_)\n",
    "#sel.fit(X_train, y_train)\n",
    "#sel.get_support()\n",
    "#selected_feat= X_train.columns[(sel.get_support())]\n",
    "#print(len(selected_feat)) \n",
    "#print(selected_feat)\n",
    "\n",
    "# rerun rf model w/out grid search\n",
    "#model = RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "## histogram of the top 10 features by importance\n",
    "# create a list of features/column names\n",
    "features = list(X_train.columns.values) \n",
    "\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)  \n",
    "feat_importances.nlargest(25).plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
