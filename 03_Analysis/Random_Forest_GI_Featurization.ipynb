{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methylation Biomarkers for Predicting Cancer**\n",
    "\n",
    "## **Random Forest for Feature Selection**\n",
    "\n",
    "**Author:** Meg Hutch\n",
    "\n",
    "**Date:** February 14, 2020\n",
    "\n",
    "**Objective:** Use random forest to select genes for features in our deep learning classifier.\n",
    "\n",
    "**Update**: Could try and choose a classifier maybe based off highest AUC and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_score, recall_score, precision_recall_fscore_support, f1_score, log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory for git hub\n",
    "import os\n",
    "#os.chdir('/home/mrh1996/')\n",
    "os.chdir('C:\\\\Users\\\\User\\\\Box Sync/Projects/')\n",
    "os. getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = pd.read_csv('Multi_Cancer_DL/02_Processed_Data/GI_Datasets/mcTrain_x_gi_Full_70_30.csv')\n",
    "mcTrain_y = pd.read_csv('Multi_Cancer_DL/02_Processed_Data/GI_Datasets/mcTrain_y_gi_Full_70_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Un-neccessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.drop(columns=[\"dilute_library_concentration\", \"age\", \"gender\", \"frag_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert seq_num id to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcTrain_x = mcTrain_x.set_index('id')\n",
    "mcTrain_y = mcTrain_y.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training Data into a training/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(21420)\n",
    "X_train, X_test, y_train, y_test = train_test_split(mcTrain_x, mcTrain_y, test_size=0.25, random_state=25, shuffle = True, stratify = mcTrain_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Disease Distributions After Training/Testing Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perc = y_train.groupby(['diagnosis']).size()/len(y_train)*100\n",
    "y_test_perc = y_test.groupby(['diagnosis']).size()/len(y_test)*100\n",
    "\n",
    "#print(y_train_perc)\n",
    "#print(y_test_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encode y classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "y_train_multi = preprocessing.label_binarize(y_train, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "y_test_multi = preprocessing.label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save copy of X_train - this will be used for feature selection down the line\n",
    "X_train_orig = X_train\n",
    "\n",
    "# Convert all to arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values \n",
    "y_test = y_test.values\n",
    "\n",
    "# Convert y_train to 1D\n",
    "y_train = y_train.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "The hyperparameter tuning function was adapted from Garrett's modeling lecture:\n",
    "\n",
    "https://github.com/geickelb/HSIP442_guest_lecture/blob/master/notebooks/modeling.ipynb\n",
    "\n",
    "scoring parameter for multi-classification: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter (will try f1_samples and precision_samples and/or just accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define scoring function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring = ['AUC', 'Accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Random Forest Hypertuning Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning_fxn(X, y, nfolds, model , param_grid, scoring = 'roc_auc_ovo_weighted', verbose=True, cv = True, \n",
    "                    return_train_score = True): \n",
    "    \"\"\"function that uses GridSearchCV to test a specified param_grid of hyperparameters and choose the optimal one based on nfolds cross-validation results. \n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- a 'fitted' sklearn model object \n",
    "    X -- predictor matrix (dtype='numpy array', required)\n",
    "    y -- outcome vector (dtype='numpy array', required)\n",
    "    cv -- if True, prints a the roc_auc score from 10-fold crossvalidation (dtype='boolean', default='True')\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(12345)\n",
    "    \n",
    "    # The scorers can be either be one of the predefined metric strings or a scorer\n",
    "    # callable, like the one returned by make_scorer\n",
    "    #scoring = {'AUC': 'roc_auc_ovr', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator= model,\n",
    "                                     param_grid=param_grid,\n",
    "                                     cv=StratifiedKFold(nfolds), # stratified k-folds will preserve class balances - this function is what got the rest of the code to work with roc validation\n",
    "                                     scoring=scoring,\n",
    "                                     return_train_score=True,\n",
    "                                     n_jobs = -1)\n",
    "    \n",
    "    #scoring = {'accuracy': 'accuracy', 'auc': 'roc_auc_ovr'}\n",
    "    #scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "    \n",
    "    #OneVsRestClassifier(grid_search.fit(X, y))   \n",
    "    grid_search.fit(X, y)\n",
    "    print(\" scorer function: {}\".format(scoring))\n",
    "    print(\" ##### CV performance: mean & sd scores #####\")\n",
    "\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    \n",
    "    print('best cv score: {:0.3f}'.format(grid_search.best_score_))\n",
    "    print('best cv params: ', grid_search.best_params_)\n",
    "\n",
    "    worst_index=np.argmin(grid_search.cv_results_['mean_test_score'])\n",
    "    print('worst cv score: {:0.3f}'.format(grid_search.cv_results_['mean_test_score'][worst_index]))\n",
    "    print('worst cv params: ', grid_search.cv_results_['params'][worst_index])\n",
    "    ##\n",
    "    if verbose==True:\n",
    "        for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    \n",
    "    return(grid_search)\n",
    "    #print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning RF hyperparameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100, 300, 500, 1000] #  100, 300, 500, 1000\n",
    "# Number of features to consider at every split\n",
    "max_features = [3, 10, 'auto'] # 'auto' which is equivalent to sqrt(n_features)\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 5, 10, 15]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper=hypertuning_fxn(X_train, y_train, nfolds=10, model=model , param_grid=param_grid, scoring= 'roc_auc_ovo_weighted', cv = True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the Best Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_hyper.best_estimator_)\n",
    "rf = rf_hyper.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rf_hyper.classes_) # number of classes - this is an attribute of the rf classifier\n",
    "\n",
    "# predict the probabilities of the classifier when applied to the test set - Note: [:,] shapes data into the right format for multiclass\n",
    "rf_probs = rf_hyper.best_estimator_.predict_proba(X_test)[:,]\n",
    "#rf_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_hyper.best_estimator_.predict(X_test)[:,]\n",
    "#rf_preds = rf_preds.reshape(-1,1).shape\n",
    "#rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** ovr computes the AUC of each class against the rest [3] [4]. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when average == 'macro', because class imbalance affects the composition of each of the ‘rest’ groupings\"\n",
    "\n",
    "Weighted: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc_score(y_test, rf_probs, multi_class = 'ovr', average = 'weighted') # multi_class must be in ('ovo', 'ovr')\n",
    "\n",
    "print('Roc_Auc_Score:')\n",
    "roc_auc_score(y_test_multi, rf_probs, multi_class = 'ovo', average = 'weighted') # multi_class must be in ('ovo', 'ovr')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_hyper.best_estimator_.predict(X_test)[:,]\n",
    "confusion_matrix(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['HEA', 'CRC', 'ESCA', 'HCC', 'STAD', 'GBM', 'BRCA']\n",
    "print(classification_report(y_test, rf_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Important Features**\n",
    "\n",
    "Feature Importance for Multi-class classification:\n",
    "https://stackoverflow.com/questions/54562464/can-i-show-feature-importance-for-multioutputclassifier\n",
    "\n",
    "MultiOutputClassifier objects have an attribute called estimators_. If you run multi_forest.estimators_, you will get a list containing an object for each of your RandomForest classifiers.\n",
    "\n",
    "For each of these RandomForest classifier objects, you can access its feature importances through the feature_importances_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "feat_impts = [] \n",
    "# bind all rf estimators for each classifier (each multi-class output - in our case 7)\n",
    "for clf in rf.estimators_:\n",
    "    feat_impts.append(clf.feature_importances_)\n",
    "\n",
    "# calculate the mean of features across genes\n",
    "feat = np.mean(feat_impts, axis=0)\n",
    "# create a list of features (gene names)\n",
    "features = list(X_train_orig.columns.values) \n",
    "# add gene names to the means\n",
    "feat_importances = pd.Series(feat, index=X_train_orig.columns)  \n",
    "\n",
    "# plot feature importance for nlargest means \n",
    "feat_importances.nlargest(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save List of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_100 = feat_importances.nlargest(100)\n",
    "feat_100\n",
    "feat_100.to_csv('Multi_Cancer_DL/02_Processed_Data/Final_Datasets/rf_100feats_FULL_gi_70_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
